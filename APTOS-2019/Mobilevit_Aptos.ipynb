{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ----------------------------\n# Config\n# ----------------------------\nIMG_SIZE = 380\nBATCH_SIZE = 32\nEPOCHS = 25\nLR = 1e-4\nSEED = 42\n\n# âœ… Updated paths to APTOS dataset\nDATA_CSV = \"/kaggle/input/aptos2019-blindness-detection/train.csv\"\nIMG_DIR = \"/kaggle/input/aptos2019-blindness-detection/train_images\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntorch.manual_seed(SEED)\n\n# ----------------------------\n# Dataset (APTOS)\n# ----------------------------\nclass APTOSDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_id = row['id_code'] + \".png\"   # âœ… APTOS filenames end in .png\n        label = int(row['diagnosis'])\n        img_path = os.path.join(self.img_dir, img_id)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# ----------------------------\n# Data Prep\n# ----------------------------\ndf = pd.read_csv(DATA_CSV)\n\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"diagnosis\"],\n    random_state=SEED\n)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = APTOSDataset(train_df, IMG_DIR, transform=train_transform)\nval_dataset = APTOSDataset(val_df, IMG_DIR, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                        num_workers=4, pin_memory=True)\n\n# ----------------------------\n# Model\n# ----------------------------\nmodel = models.resnet101(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 5)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\nmodel = model.to(DEVICE)\n\n# ----------------------------\n# Loss and Optimizer\n# ----------------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        if self.reduction == 'mean':\n            return loss.mean()\n        else:\n            return loss.sum()\n\ncriterion = FocalLoss()\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\nscaler = torch.cuda.amp.GradScaler()\n\n# ----------------------------\n# Training Loop\n# ----------------------------\nbest_acc = 0.0\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss, train_preds, train_targets = 0, [], []\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n\n    for imgs, labels in train_bar:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n        preds = outputs.argmax(1).detach().cpu().numpy()\n        train_preds.extend(preds)\n        train_targets.extend(labels.cpu().numpy())\n        train_bar.set_postfix(loss=loss.item())\n\n    train_acc = accuracy_score(train_targets, train_preds)\n\n    model.eval()\n    val_loss, val_preds, val_targets = 0, [], []\n    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n    with torch.no_grad():\n        for imgs, labels in val_bar:\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            with torch.cuda.amp.autocast():\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            preds = outputs.argmax(1).detach().cpu().numpy()\n            val_preds.extend(preds)\n            val_targets.extend(labels.cpu().numpy())\n            val_bar.set_postfix(loss=loss.item())\n\n    val_acc = accuracy_score(val_targets, val_preds)\n    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n          f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n          f\"Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n          f\"Val Acc: {val_acc:.4f}\")\n\n    scheduler.step()\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), \"/kaggle/working/best_resnet101.pth\")\n\n# ----------------------------\n# Evaluation\n# ----------------------------\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_resnet101.pth\", map_location=DEVICE))\nmodel.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nacc = accuracy_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds, average=\"weighted\")\nprecision = precision_score(all_labels, all_preds, average=\"weighted\")\nrecall = recall_score(all_labels, all_preds, average=\"weighted\")\ncm = confusion_matrix(all_labels, all_preds)\n\nprint(f\"\\nðŸ“Š Accuracy: {acc:.4f}\")\nprint(f\"ðŸ“Š F1 Score: {f1:.4f}\")\nprint(f\"ðŸ“Š Precision: {precision:.4f}\")\nprint(f\"ðŸ“Š Recall: {recall:.4f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[0,1,2,3,4], yticklabels=[0,1,2,3,4])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"ResNet101 Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T16:36:14.310822Z","iopub.execute_input":"2025-10-03T16:36:14.311071Z","iopub.status.idle":"2025-10-03T16:48:51.546761Z","shell.execute_reply.started":"2025-10-03T16:36:14.311042Z","shell.execute_reply":"2025-10-03T16:48:51.545552Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:00<00:00, 205MB/s] \n/tmp/ipykernel_36/3383551989.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1/25 [Train]:   0%|          | 0/92 [00:00<?, ?it/s]/tmp/ipykernel_36/3383551989.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1/25 [Val]:   0%|          | 0/23 [00:00<?, ?it/s]                       /tmp/ipykernel_36/3383551989.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25 | Train Loss: 0.4020, Train Acc: 0.7289 | Val Loss: 0.2959, Val Acc: 0.7817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25 [Train]:   0%|          | 0/92 [00:00<?, ?it/s]/tmp/ipykernel_36/3383551989.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 2/25 [Val]:   0%|          | 0/23 [00:00<?, ?it/s]                        /tmp/ipykernel_36/3383551989.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/25 | Train Loss: 0.2660, Train Acc: 0.7986 | Val Loss: 0.2835, Val Acc: 0.7981\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25 [Train]:   0%|          | 0/92 [00:00<?, ?it/s]/tmp/ipykernel_36/3383551989.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 3/25 [Val]:   0%|          | 0/23 [00:00<?, ?it/s]                        /tmp/ipykernel_36/3383551989.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/25 | Train Loss: 0.2274, Train Acc: 0.8238 | Val Loss: 0.2481, Val Acc: 0.7954\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25 [Train]:   0%|          | 0/92 [00:00<?, ?it/s]/tmp/ipykernel_36/3383551989.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n                                                                                \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3383551989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}