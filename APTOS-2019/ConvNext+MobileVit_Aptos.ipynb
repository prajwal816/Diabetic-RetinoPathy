{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# hybrid_supcon_234_balanced_viz_updated.py\n\"\"\"\nSupCon pretrain on classes [2,3,4] with balanced batches + weighted SupCon loss,\nthen fine-tune whole hybrid model on all 5 classes with MixUp + Focal Loss,\ngradual unfreeze and multi-GPU DataParallel for the classifier stage.\n\nBackbone: ConvNeXt (convnext_base) + MobileViT (mobilevitv2_100)\n(ResNet removed)\n\nAdded:\n- ROC-AUC curves per class\n- Predictions visualization on validation images\n- Epoch vs loss & accuracy plots\n\"\"\"\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay\nimport timm\nimport matplotlib.pyplot as plt\n\n# ----------------------------\n# CONFIG\n# ----------------------------\nCSV_PATH = \"/kaggle/input/aptos2019-blindness-detection/train.csv\"\nIMG_DIR = \"/kaggle/input/aptos2019-blindness-detection/train_images\"\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# SupCon stage\nSUPCON_CLASSES = [1, 2, 3, 4]\nSUPCON_IMG = 224\nSUPCON_EPOCHS = 50\nSUPCON_BPC = 4\nSUPCON_LR = 1e-4\nSUPCON_CLASS_WEIGHTS = torch.tensor([1.5, 1.0, 1.5, 1.5])  # emphasize classes (relative)\n\n# Classifier stage\nIMG_SIZE = 380\nBATCH_SIZE = 16\nEPOCHS = 40\nLR = 1e-4\nUSE_AMP = True\nMIXUP_ALPHA = 0.4\nSAVE_PATH = \"best_hybrid_supcon234_finetune.pth\"\nNUM_WORKERS = 2\nPIN_MEMORY = True\n\n# Freeze schedule now targets convnext and mobilevit blocks/stages only\nFREEZE_SCHEDULE = {\n    2: [],\n    5: [\"model_cnx.stages.3\", \"model_mv.blocks.11\"],\n    8: [\"model_cnx.stages.2\", \"model_mv.blocks.10\"],\n    12: [\"model_cnx.stages.1\", \"model_mv.blocks.9\"]\n}\nFOCAL_GAMMA = 2.0\n\n# ----------------------------\n# Dataset\n# ----------------------------\nclass APTOSDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, label_col=\"diagnosis\"):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.label_col = label_col\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, f\"{row['id_code']}.png\")\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        label = int(row[self.label_col])\n        return img, label\n\n# ----------------------------\n# Balanced Batch Sampler\n# ----------------------------\nclass BalancedBatchSampler(Sampler):\n    \"\"\"\n    groups: list of lists of indices, one list per class\n    bpc: images per class in a batch\n    Yields lists of indices (a full batch)\n    \"\"\"\n    def __init__(self, groups, bpc):\n        super().__init__(None)\n        self.groups = [list(g)[:] for g in groups]\n        self.bpc = int(bpc)\n        self.num_classes = len(groups)\n        # number of batches limited by smallest pool\n        self._batches_per_epoch = min(len(g) // self.bpc for g in self.groups) if len(self.groups) > 0 else 0\n\n    def __iter__(self):\n        pools = [g[:] for g in self.groups]\n        for p in pools:\n            random.shuffle(p)\n        ptr = [0] * self.num_classes\n        for _ in range(self._batches_per_epoch):\n            batch = []\n            for c in range(self.num_classes):\n                start = ptr[c]\n                batch.extend(pools[c][start:start + self.bpc])\n                ptr[c] += self.bpc\n            random.shuffle(batch)\n            yield batch\n\n    def __len__(self):\n        return self._batches_per_epoch\n\n# ----------------------------\n# Models\n# ----------------------------\nclass HybridModel(nn.Module):\n    def __init__(self, num_classes=5, pretrained=True):\n        super().__init__()\n        # MobileViT backbone\n        self.model_mv = timm.create_model(\"mobilevitv2_100\", pretrained=pretrained, num_classes=0, global_pool=\"avg\")\n        # ConvNeXt backbone (keep convnext_base as requested)\n        self.model_cnx = timm.create_model(\"convnext_base\", pretrained=pretrained, num_classes=0, global_pool=\"avg\")\n\n        f1 = getattr(self.model_mv, \"num_features\", None)\n        f3 = getattr(self.model_cnx, \"num_features\", None)\n\n        # fallback: timm sometimes uses .num_features, sometimes .num_features? but getattr handles it.\n        if f1 is None:\n            try:\n                f1 = self.model_mv.num_features\n            except Exception:\n                f1 = self.model_mv.feature_info.channels()[-1]\n        if f3 is None:\n            try:\n                f3 = self.model_cnx.num_features\n            except Exception:\n                f3 = self.model_cnx.feature_info.channels()[-1]\n\n        total_features = int(f1) + int(f3)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(total_features, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x, return_features=False):\n        f1 = self.model_mv(x)\n        f3 = self.model_cnx(x)\n        fused = torch.cat([f1, f3], dim=1)\n        if return_features:\n            return fused\n        return self.classifier(fused)\n\n# ----------------------------\n# Losses\n# ----------------------------\nclass WeightedSupConLoss(nn.Module):\n    def __init__(self, temperature=0.07, class_weights=None):\n        super().__init__()\n        self.temperature = temperature\n        self.class_weights = class_weights.float() if class_weights is not None else None\n\n    def forward(self, features, labels):\n        device = features.device\n        B = features.shape[0]\n        features = F.normalize(features, dim=1)\n        labels = labels.view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n\n        logits = torch.matmul(features, features.T) / self.temperature\n        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n        logits = logits - logits_max.detach()\n\n        diag_mask = torch.eye(B, device=device)\n        exp_logits = torch.exp(logits) * (1 - diag_mask)\n\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n\n        positives = (mask - diag_mask)\n        denom = positives.sum(1) + 1e-9\n        mean_log_prob_pos = (positives * log_prob).sum(1) / denom\n\n        if self.class_weights is not None:\n            # labels.squeeze might be long, ensure index mapping safe\n            sample_weights = self.class_weights[labels.squeeze().long()].to(device)\n            loss = -(sample_weights * mean_log_prob_pos).mean()\n        else:\n            loss = -mean_log_prob_pos.mean()\n        return loss\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None, reduction=\"mean\"):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = torch.tensor(alpha, dtype=torch.float) if alpha is not None else None\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        device = inputs.device\n        logpt = F.log_softmax(inputs, dim=1)\n        pt = torch.exp(logpt)\n        targets = targets.long()\n        logpt_t = logpt.gather(1, targets.unsqueeze(1)).squeeze(1)\n        pt_t = pt.gather(1, targets.unsqueeze(1)).squeeze(1)\n        if self.alpha is not None:\n            alpha_t = self.alpha.to(device).gather(0, targets)\n            loss = -alpha_t * ((1 - pt_t) ** self.gamma) * logpt_t\n        else:\n            loss = -((1 - pt_t) ** self.gamma) * logpt_t\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        else:\n            return loss\n\n# ----------------------------\n# MixUp\n# ----------------------------\ndef mixup_data(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1.0 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1.0 - lam) * criterion(pred, y_b)\n\n# ----------------------------\n# Data prep\n# ----------------------------\ndf = pd.read_csv(CSV_PATH)\nsupcon_df = df[df[\"diagnosis\"].isin(SUPCON_CLASSES)].reset_index(drop=True)\nsupcon_df[\"supcon_label\"] = supcon_df[\"diagnosis\"].map({c: i for i, c in enumerate(SUPCON_CLASSES)})\n\nsupcon_transform = transforms.Compose([\n    transforms.RandomResizedCrop(SUPCON_IMG, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.2, 0.2, 0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nsupcon_dataset = APTOSDataset(supcon_df, IMG_DIR, transform=supcon_transform, label_col=\"supcon_label\")\n\nindices_per_class = []\nfor class_idx in range(len(SUPCON_CLASSES)):\n    idxs = supcon_df.index[supcon_df[\"supcon_label\"] == class_idx].tolist()\n    indices_per_class.append(idxs)\n\nbatch_sampler = BalancedBatchSampler(indices_per_class, bpc=SUPCON_BPC)\nsupcon_loader = DataLoader(supcon_dataset, batch_sampler=batch_sampler, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\n# ----------------------------\n# SupCon Training\n# ----------------------------\nmodel_supcon = HybridModel(num_classes=5, pretrained=True).to(DEVICE)\nsupcon_loss_fn = WeightedSupConLoss(temperature=0.07, class_weights=SUPCON_CLASS_WEIGHTS.to(DEVICE))\noptimizer_supcon = optim.AdamW(model_supcon.parameters(), lr=SUPCON_LR, weight_decay=1e-4)\nscaler_supcon = torch.amp.GradScaler(enabled=True)\n\nprint(\"=== SupCon pretraining on classes\", SUPCON_CLASSES, \"===\")\nfor epoch in range(SUPCON_EPOCHS):\n    model_supcon.train()\n    running_loss = 0.0\n    loop = tqdm(supcon_loader, desc=f\"SupCon Epoch {epoch+1}/{SUPCON_EPOCHS}\")\n    for batch in loop:\n        # batch is a list of indices produced by sampler -> DataLoader will collate into list of samples\n        imgs, labels = batch  # if DataLoader returns tuples normally; but because we used batch_sampler, DataLoader will fetch samples normally\n        # However, some backends may produce lists; this pattern keeps compatibility assuming DataLoader collate did the job.\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n\n        optimizer_supcon.zero_grad()\n        with torch.amp.autocast(device_type=\"cuda\", enabled=(DEVICE==\"cuda\")):\n            feats = model_supcon(imgs, return_features=True)\n            loss = supcon_loss_fn(feats, labels)\n        scaler_supcon.scale(loss).backward()\n        scaler_supcon.step(optimizer_supcon)\n        scaler_supcon.update()\n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    avg_loss = running_loss / max(1, len(supcon_loader))\n    print(f\"SupCon Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n\ntorch.save(model_supcon.state_dict(), \"hybrid_supcon234_pretrained.pth\")\nprint(\"Saved SupCon weights.\")\n\n# ----------------------------\n# Classifier Training\n# ----------------------------\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"diagnosis\"], random_state=SEED)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\ntrain_dataset = APTOSDataset(train_df, IMG_DIR, transform=train_transform)\nval_dataset = APTOSDataset(val_df, IMG_DIR, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\nmodel = HybridModel(num_classes=5, pretrained=True).to(DEVICE)\nstate = torch.load(\"hybrid_supcon234_pretrained.pth\", map_location=DEVICE)\n# load_supcon weights into backbones - strict=False to allow classifier mismatch\nmodel.load_state_dict(state, strict=False)\n\n# compute class weights for focal alpha\nclass_counts = train_df[\"diagnosis\"].value_counts().sort_index().values\nclass_weights = (class_counts.sum() / (len(class_counts) * class_counts)).astype(np.float32)\nalpha = torch.tensor(class_weights, dtype=torch.float32)\ncriterion_cls = FocalLoss(gamma=FOCAL_GAMMA, alpha=alpha)\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\nscaler = torch.amp.GradScaler(enabled=USE_AMP)\n\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs.\")\n    model = nn.DataParallel(model)\n\n# By default freeze everything except classifier initially\nfor name, p in model.named_parameters():\n    p.requires_grad = (\"classifier\" in name)\n\ndef gradual_unfreeze(model, epoch, schedule):\n    if epoch in schedule:\n        patterns = schedule[epoch]\n        for name, p in model.named_parameters():\n            if any(pat in name for pat in patterns):\n                p.requires_grad = True\n        print(f\"[Unfreeze] Epoch {epoch}: {patterns}\")\n\n# ----------------------------\n# Track metrics for plotting\n# ----------------------------\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nbest_macro_f1 = -1.0\nprint(\"=== Classifier training ===\")\nfor epoch in range(EPOCHS):\n    gradual_unfreeze(model, epoch, FREEZE_SCHEDULE)\n\n    model.train()\n    running_loss = 0.0\n    preds_all, targets_all = [], []\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        imgs_mixed, ya, yb, lam = mixup_data(imgs, labels)\n        with torch.amp.autocast(device_type=\"cuda\", enabled=(DEVICE==\"cuda\" and USE_AMP)):\n            outputs = model(imgs_mixed)\n            loss = mixup_criterion(criterion_cls, outputs, ya, yb, lam)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        preds_all.extend(outputs.argmax(1).detach().cpu().numpy())\n        targets_all.extend(labels.cpu().numpy())\n    train_acc = accuracy_score(targets_all, preds_all)\n    train_loss_avg = running_loss / max(1, len(train_loader))\n    train_losses.append(train_loss_avg)\n    train_accs.append(train_acc)\n\n    model.eval()\n    val_preds, val_targets = [], []\n    val_outputs_all = []\n    val_loss_accum = 0.0\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            with torch.amp.autocast(device_type=\"cuda\", enabled=(DEVICE==\"cuda\" and USE_AMP)):\n                outputs = model(imgs)\n                loss_v = criterion_cls(outputs, labels)\n            val_loss_accum += loss_v.item()\n            val_preds.extend(outputs.argmax(1).cpu().numpy())\n            val_targets.extend(labels.cpu().numpy())\n            val_outputs_all.append(outputs.softmax(dim=1).cpu())\n    val_loss_avg = val_loss_accum / max(1, len(val_loader))\n    val_losses.append(val_loss_avg)\n    val_acc = accuracy_score(val_targets, val_preds)\n    val_accs.append(val_acc)\n\n    from sklearn.metrics import f1_score\n    macro_f1 = f1_score(val_targets, val_preds, average=\"macro\")\n    print(f\"Epoch {epoch+1} | Train Loss {train_loss_avg:.4f} Acc {train_acc:.4f} | Val Loss {val_loss_avg:.4f} Acc {val_acc:.4f} | F1 {macro_f1:.4f}\")\n\n    if macro_f1 > best_macro_f1:\n        best_macro_f1 = macro_f1\n        if isinstance(model, nn.DataParallel):\n            torch.save(model.module.state_dict(), SAVE_PATH)\n        else:\n            torch.save(model.state_dict(), SAVE_PATH)\n        print(f\"Saved best model (F1={macro_f1:.4f})\")\n\n    scheduler.step()\n\n# ----------------------------\n# Plot metrics\n# ----------------------------\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(range(1,EPOCHS+1), train_losses, label=\"Train Loss\")\nplt.plot(range(1,EPOCHS+1), val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs Epoch\")\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(range(1,EPOCHS+1), train_accs, label=\"Train Acc\")\nplt.plot(range(1,EPOCHS+1), val_accs, label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs Epoch\")\nplt.legend()\nplt.show()\n\n# ----------------------------\n# Final eval & ROC-AUC curves\n# ----------------------------\nbest_state = torch.load(SAVE_PATH, map_location=DEVICE)\nfinal_model = HybridModel(num_classes=5, pretrained=True).to(DEVICE)\nfinal_model.load_state_dict(best_state, strict=False)\nfinal_model.eval()\n\nval_preds, val_targets = [], []\nval_probs = []\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        outputs = final_model(imgs)\n        val_preds.extend(outputs.argmax(1).cpu().numpy())\n        val_targets.extend(labels.cpu().numpy())\n        val_probs.append(outputs.softmax(dim=1).cpu())\nval_probs = torch.cat(val_probs, dim=0).numpy()\nval_targets_np = np.array(val_targets)\n\nprint(\"\\nFinal Report:\")\nprint(classification_report(val_targets, val_preds, digits=4))\ncm = confusion_matrix(val_targets, val_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(5)))\ndisp.plot(cmap=\"Blues\", xticks_rotation=45)\nplt.show()\n\n# ROC-AUC per class\nplt.figure(figsize=(10,8))\nfor i in range(5):\n    RocCurveDisplay.from_predictions((val_targets_np == i).astype(int), val_probs[:, i], name=f\"Class {i}\")\nplt.plot([0,1],[0,1],\"k--\")\nplt.title(\"ROC Curves per Class\")\nplt.show()\n\n# ----------------------------\n# Visualize predictions on validation images\n# ----------------------------\ndef show_predictions(model, dataset, num_images=8):\n    model.eval()\n    indices = np.random.choice(len(dataset), size=num_images, replace=False)\n    plt.figure(figsize=(15, 5))\n    for i, idx in enumerate(indices):\n        img, label = dataset[idx]\n        inp = img.unsqueeze(0).to(DEVICE)\n        with torch.no_grad():\n            out = model(inp)\n            pred = out.argmax(1).item()\n        img_np = img.permute(1, 2, 0).cpu().numpy()\n        img_np = np.clip(img_np * np.array([0.229,0.224,0.225]) + np.array([0.485,0.456,0.406]), 0,1)\n        plt.subplot(2, num_images//2, i+1)\n        plt.imshow(img_np)\n        plt.title(f\"GT: {label} | Pred: {pred}\")\n        plt.axis(\"off\")\n    plt.show()\n\nshow_predictions(final_model, val_dataset, num_images=8)\n\ntorch.save(final_model.state_dict(), SAVE_PATH.replace(\".pth\", \"_final.pth\"))\nprint(\"Final model saved.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T20:40:12.003811Z","iopub.execute_input":"2025-10-03T20:40:12.004022Z","iopub.status.idle":"2025-10-03T20:42:17.779053Z","shell.execute_reply.started":"2025-10-03T20:40:12.004002Z","shell.execute_reply":"2025-10-03T20:42:17.777997Z"}},"outputs":[],"execution_count":null}]}
