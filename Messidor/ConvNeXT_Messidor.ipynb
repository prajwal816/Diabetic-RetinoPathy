{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2819730,"sourceType":"datasetVersion","datasetId":1723812}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\nimport pandas as pd\nfrom PIL import Image\nimport timm\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ----------------------------\n# Config\n# ----------------------------\nIMG_SIZE = 380\nBATCH_SIZE = 32\nEPOCHS = 20\nLR = 1e-4   # lowered learning rate\nSEED = 42\n\nDATA_CSV = \"/kaggle/input/messidor2preprocess/messidor_data.csv\"\nIMG_DIR = \"/kaggle/input/messidor2preprocess/messidor-2/messidor-2/preprocess\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntorch.manual_seed(SEED)\n\n# ----------------------------\n# Dataset\n# ----------------------------\nclass MessidorDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_id = row['id_code']       # already includes \".png\"\n        label = int(row['diagnosis'])\n        img_path = os.path.join(self.img_dir, img_id)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\nif __name__ == \"__main__\":\n    # ----------------------------\n    # Data Prep\n    # ----------------------------\n    df = pd.read_csv(DATA_CSV)\n\n    # âœ… Ensure labels are ints 0â€“4\n    if df[\"diagnosis\"].dtype != \"int64\" and df[\"diagnosis\"].dtype != \"int32\":\n        label_map = {cls: i for i, cls in enumerate(sorted(df[\"diagnosis\"].unique()))}\n        df[\"diagnosis\"] = df[\"diagnosis\"].map(label_map)\n\n    print(\"Unique labels:\", df[\"diagnosis\"].unique())\n    print(\"Label distribution:\\n\", df[\"diagnosis\"].value_counts())\n\n    train_df, val_df = train_test_split(\n        df, test_size=0.2, stratify=df[\"diagnosis\"], random_state=SEED\n    )\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    train_dataset = MessidorDataset(train_df, IMG_DIR, transform=train_transform)\n    val_dataset = MessidorDataset(val_df, IMG_DIR, transform=val_transform)\n\n    # âŒ Disabled WeightedRandomSampler for now\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                              num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                            num_workers=4, pin_memory=True)\n\n    # ----------------------------\n    # Model\n    # ----------------------------\n    model = timm.create_model(\"convnext_base\", pretrained=True, num_classes=5)\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel.\")\n        model = nn.DataParallel(model)\n    model = model.to(DEVICE)\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n    scaler = torch.cuda.amp.GradScaler()\n\n    # ----------------------------\n    # Training\n    # ----------------------------\n    best_acc = 0.0\n    print(f\"Starting training for {EPOCHS} epochs. Steps/epoch={len(train_loader)}, LR={LR}\")\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss, train_preds, train_targets = 0, [], []\n        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n\n        for imgs, labels in train_bar:\n            imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n            optimizer.zero_grad()\n            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item()\n            preds = outputs.argmax(1).detach().cpu().numpy()\n            train_preds.extend(preds)\n            train_targets.extend(labels.cpu().numpy())\n            train_bar.set_postfix(loss=loss.item())\n\n        train_acc = accuracy_score(train_targets, train_preds)\n\n        # ----------------------------\n        # Validation\n        # ----------------------------\n        model.eval()\n        val_loss, val_preds, val_targets = 0, [], []\n        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n        with torch.no_grad():\n            for imgs, labels in val_bar:\n                imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n                with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n                    outputs = model(imgs)\n                    loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                preds = outputs.argmax(1).detach().cpu().numpy()\n                val_preds.extend(preds)\n                val_targets.extend(labels.cpu().numpy())\n                val_bar.set_postfix(loss=loss.item())\n\n        val_acc = accuracy_score(val_targets, val_preds)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n              f\"Train Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n              f\"Val Acc: {val_acc:.4f}\")\n\n        scheduler.step()\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), \"/kaggle/working/best_convnext.pth\")\n            print(f\"âœ… New best model saved. Best Val Acc: {best_acc:.4f}\")\n\n    print(f\"\\nâœ… Training complete. Best Val Acc: {best_acc:.4f}\")\n\n    # ----------------------------\n    # Evaluation\n    # ----------------------------\n    model.load_state_dict(torch.load(\"/kaggle/working/best_convnext.pth\", map_location=DEVICE))\n    model.eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n                outputs = model(imgs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"\\nðŸ“Š Accuracy: {acc:.4f}\")\n    print(f\"ðŸ“Š F1 Score: {f1:.4f}\")\n    print(f\"ðŸ“Š Precision: {precision:.4f}\")\n    print(f\"ðŸ“Š Recall: {recall:.4f}\")\n    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=[0,1,2,3,4], yticklabels=[0,1,2,3,4])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"ConvNeXt Confusion Matrix\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}